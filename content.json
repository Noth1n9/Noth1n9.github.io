{"pages":[],"posts":[{"title":"Cache","text":"&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; —— Noth1n9 缓存 缓存是位于CPU和内存之间的临时存储器，它的容量比内存小的多但是交换速度却比内存要快得多。 缓存能够解决CPU运算速度与内存读写速度不匹配的矛盾，因为CPU运算速度要比内存读写速度快很多，这样会使CPU花费很长时间等待数据。 缓存命中123456# 代码1int arr[100][100];for(int i = 0; i &lt; 100; i++) for(int j = 0; j &lt; 100; j++) arr[i][j] = 1; 123456# 代码2int arr[100][100];for(int i = 0; i &lt; 100; i++) for(int j = 0; j &lt; 100; j++) arr[j][i] = 1; 上述两段代码看似没有什么区别，但由于缓存命中的问题，会导致速度上的差异。计算机的CPU与缓存和主存之间进行数据交换具有很大的时间差异，因此能直接在缓存中找到数据时，也就是缓存命中时，能有较快的速度。 缓存由多个cache line组成，cache line是缓存和主存之间数据传输的最小单位，当把数据加载到缓存中时，那么缓存控制器会从主存中一次性加载cache line大小的数据。 因此当执行arr[0][0] = 1时，缓存控制器发现arr[0][0]不在缓存中，也就是缓存不命中，需要去主存中进行读取并将数据加载到缓存中。假设此时cache line的大小为64字节，会从主存中读取arr[0][0]到arr[0][15]的数据到内存中。当执行访问arr[0][1]时，在缓存中能找到其位置，也就是缓存命中，此时的访问速度较快。 而对于代码2，当执行arr[0][0] = 1时，缓存控制器同样发现arr[0][0]不在缓存中，需要去主存中进行读取并将数据加载到缓存中，同样也会从主存中读取arr[0][0]到arr[0][15]的数据到内存中。当第二轮读取arr[1][0]时依然会发生缓存不命中，并且一直到arr[99][0]都是不命中。那么当访问arr[0][1]会是怎么情况呢？ 此时就需要考虑缓存的大小了。如果缓存大小大于数组arr大小，缓存此时相当于缓存了整个arr数组的内容。那么后续访问其他元素，确实是缓存命中。似乎和代码1分析结果差不多。但是如果缓存的大小很小，例如只有数组一半大小，那么缓存命中率就很明显会降低。同样的缓存大小，代码1依然会获得很高的缓存命中率。","link":"/2021/04/01/cache/"},{"title":"Transformer","text":"我已经无法辨别这个世界的真假了 &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; —— Noth1n9 最近经常看到Transformer在计算机视觉各个领域中屠榜，因此突发奇想对其进行‘深入’研究。 虽然对其早有所耳闻，但也只是局限于《Attention is all you need》、Key、Value、Query等，于是‘奋发图强’打开Google，开始博览群博客。 Transformer模型框架 首先，Transformer的整体模型框架如上所示。整体同样采用了Encoder-Decoder的结构，Encoder由堆叠多个相同的Encoder模块组成，Decoder也同样如此，并且在Transformer中的Attention模块都是在self-attention的基础上形成的。 Encoder模块 Encoder模块主要由两个子模块组成：Multi-Head Attention和Feed Forward，并且在每个模块之后都有一个残差连接和正则化层。 Self-Attention 作者在原论文中将该注意力方式称为Scaled Dot-Product Attention，计算方式如上图所示。 首先，需要将模型的输入x分别映射成Query、Key和Value矩阵，这里在实现上通过将输入和三个不同的权重矩阵相乘来得到对应的Q,K和V矩阵，接着便可通过如下公式来计算自注意力的值：$$\\operatorname{Attention}(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}\\right) V$$该计算方式和Non-local中用来获得全局注意力的方式类似，先利用Query和Key矩阵之间的相关关系来表示矩阵中每个位置与其他位置的相关关系，可通过Q、K矩阵相乘得到，再通过softmax函数进行归一化。接着 使用归一化后的权重矩阵和Value矩阵相乘便可得到该注意力层的输出。 由于当Embedding维度过大时，矩阵乘法得到结果容易出现在softmax函数的饱和区，梯度较小，因此会除以$\\sqrt{d_k}$进行缩放。 Multi-Head Attention 作者在单个Self-Attention模块的基础上进行了扩展，提出了Multi-Head Attention模块，从上图中可以看出是对多个Self-Attention模块进行并行的计算处理，从对其结果进行拼接后通过一个线性层得到最终的结果。 $$ \\begin{aligned} \\operatorname{MultiHead}(Q, K, V) &=\\text { Concat }\\left(\\text { head }_{1}, \\ldots, \\text { head }_{\\mathrm{h}}\\right) W^{O} \\\\ \\quad \\text { where head }_{\\mathrm{i}} &=\\text { Attention }\\left(Q W_{i}^{Q}, K W_{i}^{K}, V W_{i}^{V}\\right) \\end{aligned} $$ 同时为了避免计算成本过多的增加，会先减少每个Self-Attention模块的大小。假设h=8，那么每个Self-Attention模块的维度也除以8。 Feed Forward Network该模块较为简单，有两层全连接组成，并在中间包含一个Relu激活层。同时，在论文中设置输入输出的维度为512，中间层的维度为2048。 单个Encoder模块的组成大概就是上面所述，通过堆叠多个Encoder模块，便可得到Transformer的Encoder输出，再将此输入到后续的Decoder中。 Decoder模块 在Decoder模块中由三个子模块组成：Masked Multi-Head Attention、Multi-Head Attention和Feed Forward，在每个模块之后也都有一个残差连接和正则化层。 Masked Multi-Head AttentionMasked Multi-Head Attention模块顾名思义就是在Encoder中的Multi-Head Attention模块基础上添加了一个Mask层。 那么为什么在Decoder中需要对Attention层加入Mask呢？ 由于在序列问题中，对于输入我们能直接获得完整的输入，但是输出的具有顺序的，我们需要逐个token预测输出，因此在预测当前输出token时，是无法使用到未来的信息，只能使用到过去信息。在训练过程中，虽然我们可以手动处理来得到成对的输入输出，但这么做会导致每个训练数据的长度不同，而在深度学习中往往是对一个batch的数据进行训练，这是就可以添加一个Mask对未来信息进行遮挡，使得数据的长度能保持一致，拼接成一个batch的数据进行训练。在实现上通过一个如下所示的Mask矩阵，再将Attention输入矩阵根据Mask中对应为0的位置设为负无穷来达到遮挡未来信息的效果。 123456789101112tensor([[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]], dtype=torch.uint8) scores = scores.masked_fill(mask == 0, -1e9) Multi-Head AttentionDecoder模块中的Multi-Head Attention计算方式也和Encoder中的计算方式相同，但数据的来源有所不同。 在Encoder模块中所有输入都是由上一个模块的同一个输出得到的，而在Decoder中， 从模型结构图也可以看出输入中的Key和Value是来自Encoder模块的，这也是Encoder和Decoder连接的主要方式(不太确定是不是唯一方式，嘻嘻)。 Feed Forward Network这里Feed Forward Network的结构可以说是和Encoder一模一样滴。 在通过堆叠的多个Decoder模块之后，通过一个线性层映射到输出语料的维度，再通过softmax函数归一化获得下一个token的概率。这样就得到了整个Transformer模型单次预测的输出。 Positional Encoding整理完Encoder和Decoder模块之后，可以发现输入在经过Embedding之后还通过一个Positional Encoding模块。 由于Transformer中的Attention模块不同于RNN、LSTM具有循环结构，在Attention计算过程中是没有使用到每个token之间的相对或绝对位置关系。因此在Encoder和Decoder的embedding层之后加入Positional Encoding对位置信息进行编码。 在论文中，作者使用sin和cos函数来对位置信息进行编码：$$\\begin{aligned}P E_{(p o s, 2 i)} &amp;=\\sin \\left(p o s / 10000^{2 i / d_{\\text {model }}}\\right) \\\\P E_{(p o s, 2 i+1)} &amp;=\\cos \\left(p o s / 10000^{2 i / d_{\\text {model }}}\\right)\\end{aligned}$$其中$pos$表示该token在句子中的位置，而$i$表示维度。因此得到的位置编码结果和经过Embedding之后的向量具有相同的维度，可直接相加。之所以选择余弦函数是为了能获得相对位置关系，这里引用原论文中的话： We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset $k$, $PEpos+k$ can be represented as a linear function of $PEpos$. 模型的输入输出虽然整个模型的结构框架已经有了详细的了解，接下来分析模型的输入输出分别是什么。 该模型在论文中主要对应的是文本翻译任务，那么我们假设一对数据： 我爱你➡I love you， 从上图中可以看出在Encoder中有一个输入，也就是输入序列：我爱你，并将其通过Embedding映射成模型输入数据。 而在Decoder中，可以看到模型的输入不仅有来自Encoder的输出，还有一个$Output$，并同时会输出当前的一个概率。由于Decoder处理的是一个序列问题，需要逐个预测出输出。因此，在预测当前token的时候，能够将先前预测得到的token序列作为Decoder的输入来预测当前的token。 那么在预测第一个输出时Decoder的输入应该是啥呢？这时就是Shifted right起作用的时候了，这时会使用一个起始符号&lt;/s&gt;或其他自定义符号作为输入来得到第一个真实的输出token。因此我们可以将整个过程整理如下： Step 1 输入：我爱你 $\\rightarrow$ Encoder 输出：Encoder特征 Step 2 输入：Encoder特征 + 起始符号&lt;/s&gt; $\\rightarrow$ Decoder 输出：产生预测 ’I‘ Step 3 输入：Encoder特征 + 起始符号&lt;/s&gt; + ’I‘ $\\rightarrow$ Decoder 输出：产生预测 ’love‘ Step 4 输入：Encoder特征 + 起始符号&lt;/s&gt; + ’I‘ + ’love‘ $\\rightarrow$ Decoder 输出：产生预测 ’you‘ 这样就得到了完整的输出序列。同时，为了缓解中间预测错误导致后续预测偏离问题，可采用Beam Search策略。 总结敬候佳音 参考连接https://zhuanlan.zhihu.com/p/82312421 https://zhuanlan.zhihu.com/p/48508221 https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html https://www.zhihu.com/question/344516091 https://www.cnblogs.com/zingp/p/11696111.html https://zhuanlan.zhihu.com/p/166608727","link":"/2021/04/07/transformer/"},{"title":"Git工具","text":"&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; —— Noth1n9 版本控制器的起源diff与patchdiff：diff用来比较两个文件或者目录之间的差异 patch：pathc是diff的反向操作，可以通过diff所生成的diff.txt差异文件做到两个原文件的互相转换 Git工具Git基本配置配置个人身份12git config --global user.name &quot;noth1n9&quot;gei config --global user.email 932874795@qq.com 这个配置信息会在Git仓库中提交的修改信息中体现，但和Gti服务器认证使用的密码或者公钥密码无关。 文本编码配置1234567#中文编码支持git config --global gui.encoding utf-8git config --global i18n.commitencoding utf-8git config --global i18n.logoutputencoding utf-8#显示路径中的中文git config --global core.quotepath utf-8 Git基本命令Git工作区域 版本库（Repository）：在工作区有一个隐藏目录.git，这个文件夹就是Git的版本库，里面存放了Git用来管理改工程的所有版本数据，也可以叫本地仓库。 工作区（Working Directory）：日常工作的代码文件或者文档所在的文件夹。 暂存区（stage）：一般存放在工程根目录.git/index文件中，所以我们也可以把暂存区叫做索引。 文件状态 已提交（committed）：该文件已经被安全地保存在本地数据库中了 已修改（modified）：修改了摸个文件，但还没有提交保存 已暂存（staged）：把已修改的文件放在下次提交时要保存的清单中 Git常用命令工程准备新建工程——git init *** 工程克隆——git clone 查看工作区查看工作区的修改内容——git diff 查看工作区文件状态——git status 文件修改后提交推送新增/删除/移动文件到暂存区——git add/ git rm/ git mv 提交更改的文件——git commit -a(all) -m(备注) --amend(修改备注信息) 推送远端仓库——git push 查看日志查看当前分支上的提交日志——git log --name_status 分支管理列出本地分支——git branch -r(远端分支) -a(所有分支) 新建分支——git branch -b /git checkout -b（checkout会新建后切换到新分支） 删除分支——git branch -d / -D(强制删除) -r(删除远端) 切换分支——git checkout -f(强制切换) 更新分支——git pull origin remote_branch:local_branch = git fetch + git merge 合并分支——git merge 撤销操作强制回退到历史结点——git reset 回退本地所有修改而未提交的——git checkout ./-filename 分支合并合并目标股分支内容到当前分支——git merge/ git rebase","link":"/2021/05/17/git/"},{"title":"","text":"[2021ICLR]AN IMAGE IS WORTH 16X16 WORDS:TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; —— Noth1n9 论文地址：https://openreview.net/pdf?id=YicbFdNTTy pytorch源码：https://github.com/lucidrains/vit-pytorch 引言ViT作为Transformer席卷计算机视觉的‘始作俑者’，于是在这里让我们来挖掘其内部的秘密。 Transformer在NLP领域中以然是一个Baseline模型，但是在计算机视觉领域中使用较多的还属注意力模型，如何完整的将Transformer模型应用到计算机视觉中也是该论文的重点所在。 Introduction虽然很多现有的模型已经借鉴了Transformer中Self-attention的思想，但本文不同的是想要直接将图像数据应用到完整的Transformer模型中，并对其做尽可能少的改变。为了实现这个，作者将图像分成多个patches，每一个patch对应着文本输入的每一个token，这样就能采用和NLP同样的输入方式。 同时Transformer的成功得益于非常大的预训练数据集，而本文通过实验表明Transformer在视觉任务中也同样依赖于大量的预训练数据。实验表明，当在ImageNet（mid）上进行预训练时，ViT模型的效果并不如ResNet，但是当在较大的数据集如ImageNet-21k和JFT-300M上进行预训练时，模型性能能打败多个任务上的sota模型。 方法 输入embedding上图为ViT模型的主要框架。首先标准的Transformer的输入为一维序列，但图像是二维的结构，因此需要进行尺寸调整。假设图片的维度为$x \\in R^{HWC}$，将其调整为$x \\in R^{N*(P^2C)}$，其中$（P，P）$表示每个图像块的大小，$N=HW/P^2$表示一共有多少个图像块，同时也是Transformer的输入序列长度。此外在NLP中往往有一个Embedding层将输入的token映射成高维特征，这里也通过一个线性层将每个拉平后的图片块特征映射到一个D维的特征空间中。 1234self.to_patch_embedding = nn.Sequential( Rearrange('b c (h p1) (w p2) -&gt; b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width), nn.Linear(patch_dim, dim), ) [class] token此外与BERT中设置的相似，在输入Embedding 的前面加入一个[class​]token，在输出的时候就可以选择这个token所对应的结果作为后续任务的特征。 尽管该token本身是没有语义信息的，但经过多层计算之后，该特征融合了其他所有输入的特征，并且和其他特征相比，没有token本身的先验信息，能更好的表示全局信息。 1234567self.cls_token = nn.Parameter(torch.randn(1, 1, dim))x = torch.cat((cls_tokens, x), dim=1)# 返回时可选择整个Transformer输出的均值或第一个[class]token所对应的结果，# 因为与其他输入token相比，这个无明显语义信息的符号会更“公平”地融合文本中各个词的语义信息，从而更好的表示整句话的语义。x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0] position embedding获得Embedding输入还需要position embedding输入来表示序列中每个元素的位置，尽管图像具有二维特性，这里作者直接使用一个一维特征来表示position embedding，然后直接将输入的embedding和position embedding相加作为Transformer的输入。 123self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))x += self.pos_embedding[:, :(n + 1)] 接着就可以通过完整的Transformer的Encoder获得特征从而应用到下游的任务中。 fine-tuning 训练 在大数据集上预训练完去小数据上进行fine-tuning训练时，移除预训练的预测head，并添加一个零输出化的$D*K$维的前馈层。 当输入图像的分辨率更高时，抱持每一个patch的大小不变，因此输入序列的长度会边长。 Transformer能够接受任意长度的输入，但position embedding就没有意义。因此对预训练好的position embedding进行二维的插值来适应不同的大小。 实验该论文只使用较少的篇幅来介绍模型，但做了非常多的实验来证明模型的有效性。作者首先在不同大小的训练集、不同规模的模型上做了大量的实验，这里主要说一下下面这个实验。 从图中可以看出，当在ImageNet较小的数据集上进行预训练时，ViT模型的性能并不如ResNet，但是随着预训练模型的扩大，ViT模型的优势逐渐展示出来，达到最好的水平。可见预训练数据集的大小对于Transformer模型的重要程度。 并且ResNet模型在数据集规模上已经有点饱和，也就是随着数据集变大，已经没有明显的提升，而ViT模型还呈现线性增加的趋势，在未来随着数据集的增加，ViT模型的性能还可能更进一步的提升。","link":"/2021/05/22/vit/"},{"title":"","text":"[2020ECCV]End-to-End Object Detection with Transformers &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; —— Noth1n9 论文地址：https://arxiv.org/pdf/2005.12872.pdf pytorch源码：https://github.com/facebookresearch/detr Intorduction该论文作为ViT的姊妹篇，将Transformer模型应用目标检测任务上。并且该方法与以往常见的目标检测方法不同， 该模型能够直接输出结果，不需要想其他目标检测方法一样使用NMS进行后处理，同时也不需要预先设置Anchor等信息。该方法和加强版的Faster-rcnn相比，在相同参数量的情况下取得了更好的效果，并且能和Faster-rcnn扩展到Mask-rcnn一样，添加一个Mask预测头就能在全景分割任务上取得具有竞争力的效果。 Method 上图为模型的主要框架，主要由三个部分组成：CNN特征提取器、Encoder-Decoder Transformer和FFN预测头。 Backbone在Backbone上自然就有很多的选择，最常规的选择就是使用ResNet，这里作者也主要使用了ResNet50和ResNet101两个版本，同时也将ResNet最后一层的dilation设置为2来增大特征的分辨率。 1234567891011class Backbone(BackboneBase): &quot;&quot;&quot;ResNet backbone with frozen BatchNorm.&quot;&quot;&quot; def __init__(self, name: str, train_backbone: bool, return_interm_layers: bool, dilation: bool): backbone = getattr(torchvision.models, name)( replace_stride_with_dilation=[False, False, dilation], pretrained=is_main_process(), norm_layer=FrozenBatchNorm2d) num_channels = 512 if name in ('resnet18', 'resnet34') else 2048 super().__init__(backbone, train_backbone, num_channels, return_interm_layers) 经过backbone之后，先使用1*1的卷积对特征进行降维，并且Transformer需要序列输入， 因此这里也将二维的图像形变成一维的向量$R^{d*HW}$。 Spatial Position Encoding由于Transformer没有空间顺序，因此需要position encoder来编码每个像素在原图像中的位置。作者采用了两种方式的position encoding：通过学习获得的结果和类似Transformer的固定值。 Learned position embedding可学习的方式就是直接让模型来学习到如何对图像每个像素的位置信息进行编码，就和神经网络一样虽然不知道具体值的含义，但是相信网络所学习到的特征是有意义的。 12self.row_embed = nn.Embedding(50, num_pos_feats)self.col_embed = nn.Embedding(50, num_pos_feats) Sine position embedding这个和Transformer中采用正余弦来编码相似，由于像素在图像中的位置是固定的，因此可以采用这种固定的方式进行编码。 1234567891011121314151617181920x = tensor_list.tensorsmask = tensor_list.maskassert mask is not Nonenot_mask = ~masky_embed = not_mask.cumsum(1, dtype=torch.float32)x_embed = not_mask.cumsum(2, dtype=torch.float32)if self.normalize: eps = 1e-6 y_embed = y_embed / (y_embed[:, -1:, :] + eps) * self.scale x_embed = x_embed / (x_embed[:, :, -1:] + eps) * self.scale dim_t = torch.arange(self.num_pos_feats, dtype=torch.float32, device=x.device) dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats) pos_x = x_embed[:, :, :, None] / dim_t pos_y = y_embed[:, :, :, None] / dim_t pos_x = torch.stack((pos_x[:, :, :, 0::2].sin(), pos_x[:, :, :, 1::2].cos()), dim=4).flatten(3) pos_y = torch.stack((pos_y[:, :, :, 0::2].sin(), pos_y[:, :, :, 1::2].cos()), dim=4).flatten(3) pos = torch.cat((pos_y, pos_x), dim=3).permute(0, 3, 1, 2) return pos Transformer 上图为DETR中Transformer的结构，可以看出和原始Transformer并没有多少修改。 同时作者希望模型输出的目标set，并且不需要NMS等后处理操作就能直接生成结果。因此作者这里会设置N个object query，每个objct query生成一个结果，每个结果对应一个目标框和类别得分。那么问题来了，每张图片的目标数目是不一样，如何设置准确的N呢？这里作者将N设置为一个固定的足够大值（100），同时将标签中不足的目标添加一个’no object‘类别，这样预测和标签就能一一对应上，从而计算loss。 12345678910111213def forward(self, src, mask, query_embed, pos_embed): # flatten NxCxHxW to HWxNxC bs, c, h, w = src.shape src = src.flatten(2).permute(2, 0, 1) pos_embed = pos_embed.flatten(2).permute(2, 0, 1) query_embed = query_embed.unsqueeze(1).repeat(1, bs, 1) mask = mask.flatten(1) tgt = torch.zeros_like(query_embed) memory = self.encoder(src, src_key_padding_mask=mask, pos=pos_embed) hs = self.decoder(tgt, memory, memory_key_padding_mask=mask, pos=pos_embed, query_pos=query_embed) return hs.transpose(1, 2), memory.permute(1, 2, 0).view(bs, c, h, w) EncoderEncoder的输入有图像经过CNN之后的特征和spatial position encoding，其中spatial position encoding会传入到每层encoder的multi-head self-attention前，并和对应的key、query相加。 DecoderDecoder的输入包括了Encoder的输出、spatial position encoding、Mask和object query。 其中object query也是一个可学习的position encoding，可以看作图像中一个区域的编码，来表示对图像中的某个位置进行预测。接着就可以并行的对每个object query进行预测。（个人愚见：object query类似selectivesearch来生成proposal，只是selective search是启发式的，对每张图像不一样，而object query是通过模型学习到，对每张图像都一样，并且不会生成特定的框，只是产生一种先验。） 此外Encoder和Decoder还有许多其他细节需要从代码中进行理解，这里不一一详细介绍。最终每个object query通过Decoder之后再通过FFN来得到类别得分和目标框，其中目标框用归一化都的中心坐标和宽高表示。 Loss由前可知，模型的输出是一个大小为N的预测集合，而标签会通过扩展到一个大小也为N的集合，因此不同于以往的目标检测优化，作者这里需要进行两步的优化。因为预测集合和标签集合需要一一对应进行匹配，因此第一步loss的优化目的是为了找到预测集合和标签集合之间最优的二分匹配。预测集合和标签集合匹配之后就可以计算每个匹配之间的分类loss和回归框loss。 第一步：找到最优的二分匹配。原理也较为简单，穷举每一种可能的匹配，计算每个匹配的loss，选择loss最小的匹配即可，公式如下所示。在具体的实现上采用匈牙利算法（知识盲区，后续再补） 第二步：计算第一步得到匹配中每对匹配的目标loss，包括分类loss和目标框loss。这里就和常见的目标检测模型相似，采用negative log-likelihood loss和box loss组成。其中box loss为了缓解不同scale目标的影响，使用l1 loss和giou loss相加。 该方法在loss上也较为复杂，具体的可以多分析分析源码。此外作者还加了一个Auxiliary decoding losses，也就是对Decoder中的每层加了一个共享权重的FFN分支做辅助loss的训练。 Experiments作者主要在coco数据集上进行了实验，并且和Faster-rcnn进行了对比。 从实验结果可以看出，该方法在和Faster-rcnn差不多FLOPS的基础上实现了略优的成绩。不过作者也表明该方法在小目标的检测上并不太理想，算是给后人改进留下一个伏笔。此外还有一系列的可视化和消融实验来证明模型的有效性，另外还加了一个Mask head来实现全景分割，并也达到了不错的效果。 但有一点值得注意的是： A single epoch takes 28 minutes, so 300 epoch training takes around 6 days on a single machine with 8 V100 cards. 而在ViT中也有这么一句： it could be trained using a standard cloud TPUv3 with 8 cores in approximately 30 days 果然，基于Transformer的改进需要大量的训练时间，这一点应该就可以劝退很多人了，是我们这些贫穷玩家完全不敢想的，但依旧无法掩盖这些作者想法的伟大之处，并成功的引领了一股热潮，Respect！","link":"/2021/06/04/detr/"},{"title":"mlp_mixer","text":"[2021Google]MLP-Mixer: An all-MLP Architecture for Vision &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; —— Noth1n9 论文地址：https://arxiv.org/pdf/2105.01601.pdf pytorch源码：https://github.com/lucidrains/mlp-mixer-pytorch Introduction又是一篇神奇的文章，这篇文章想要避开卷积和Self-attention结构的inductive bias，又回到了最初的MLP，希望通过模型来学习到更多知识。 当年CNN的提出，由于其局部响应和权值共享的特点，参数量大幅度减少，被广泛的应用在图像上。但现在算力已经得到了大幅度的提升，作者为了消除CNN设计所带来的inductive bias，回到了最原始的MLP，为此这篇文章提出了MLP-Mixer方法，并且该模型完全由MLP(Multi-layer preceptrons)组成，通过在空间和特征通道两个维度不断堆叠MLP层，实现上只需要进行矩阵乘法、特征维度变化、归一化和非线性激活层。尽管该模型的结构简单，但是在google的强大算力和超大数据集的预训练下，实现和sotaCNN模型相似的结果，并在计算消耗上更优。 Method 上图为MLP-Mixer的主要结构，可以看出和ViT相似，先将完整的图像分成多个patch，也就是将图像序列化，接着将每一个patch通过全连接层映射，随后通过多个Mixer层对特征进行处理，再通过GAP和FC分类层得到最终的结果。 pre-patch fully-connected由于MLP不同于CNN能直接处理二维的图像数据，因此需要先将图像输入转换为一维的输入。这里采用了和ViT类似的方式，先将图像分成多个不重叠的patch，再将每个patch的图像拉平成一维的向量，通过一个fully-connect层映射到hidden dimension C。值得注意的是所有的patch都是通过同一个映射层进行映射的。 这样，一张完整图像的维度就被映射成了一个二维的input table：$X \\in R^{S*C}$，其中$S=HW/P^2$表示patch的数目，相当于一个序列化数据。 Mixer layer Mixer layer为本文的重点所在， 从上图可以看出该结构主要由两个MLP层、两个Layer norm层和矩阵维度变换组成。其中两个MLP层分别对应着token-mixing MLP和channel mixer MLP，通过对特征进行维度转置实现在不同维度上进行MLP计算。 toekn mixer：token mixer是在输入特征$X$的每一列（channel）上进行的操作，也就是计算每一个不同空间位置的token之间的相关关系，对不同的token进行混合。 channel mixer：channel mixer是在输入特征$X$的每一行（token）上进行的操作，也就是计算每一个token不同channel之间的相关关系，对不同的channel进行混合。 此外，值得注意的是虽然MLP Mixer没有使用CNN，但是skip-connection依旧被保留下来了，牛逼！接着，每一个MLP层内部结构如上图所示，包含了两层FC和一层GELU激活层。 整个Mixer layer 的计算公式如下所示： $$ \\begin{aligned} U_{*,i}=X_{*,i}+W_2\\sigma(W_1LayerNorm(X)_{*,i}), for\\ i = 1...C, \\\\ Y_{j,*}=U_{j,*}+W_4\\sigma(W_3LayerNorm(U)_{j,*}), for\\ j = 1...S. \\end{aligned} $$ 由于每个MLP有两个FC层，因此可以控制中间的隐藏层的维度，在token mixer和channel mixer中分别有两个参数进行控制：$D_s$，$D_c$。另外，从公式也可以看出，每层fully-connect是对特征中的每行(列)单独进行的，但是权重是共享的。 最终，MLP-Mixer通过堆叠多个相同输入大小的Mixer layer实现，在以往常见的CNN模型中大多使用金字塔的结构，而本文的方法和ViT类似采用固定大小的方式进行。但于ViT不同的一点的是该方法并没有使用position encoding层，因为在token mixer层中能够对所有输入序列进行全连接，这是对位置敏感的，作者认为这样就能直接学习到位置信息。 Experiments本文的实验部分也是和ViT类似的分格，对于方法的描述篇幅较少，但是进行了非常大量的实验，主要从不同大小规模的模型、预训练数据集和下游分类任务进行。","link":"/2021/06/06/mlp-mixer/"}],"tags":[],"categories":[{"name":"操作系统","slug":"操作系统","link":"/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"name":"深度学习","slug":"深度学习","link":"/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"自然语言处理","slug":"深度学习/自然语言处理","link":"/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"},{"name":"语言工具","slug":"语言工具","link":"/categories/%E8%AF%AD%E8%A8%80%E5%B7%A5%E5%85%B7/"},{"name":"论文笔记","slug":"论文笔记","link":"/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"}]}