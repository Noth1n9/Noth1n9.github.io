{"pages":[],"posts":[{"title":"Cache","text":"&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; —— Noth1n9 缓存 缓存是位于CPU和内存之间的临时存储器，它的容量比内存小的多但是交换速度却比内存要快得多。 缓存能够解决CPU运算速度与内存读写速度不匹配的矛盾，因为CPU运算速度要比内存读写速度快很多，这样会使CPU花费很长时间等待数据。 缓存命中123456# 代码1int arr[100][100];for(int i = 0; i &lt; 100; i++) for(int j = 0; j &lt; 100; j++) arr[i][j] = 1; 123456# 代码2int arr[100][100];for(int i = 0; i &lt; 100; i++) for(int j = 0; j &lt; 100; j++) arr[j][i] = 1; 上述两段代码看似没有什么区别，但由于缓存命中的问题，会导致速度上的差异。计算机的CPU与缓存和主存之间进行数据交换具有很大的时间差异，因此能直接在缓存中找到数据时，也就是缓存命中时，能有较快的速度。 缓存由多个cache line组成，cache line是缓存和主存之间数据传输的最小单位，当把数据加载到缓存中时，那么缓存控制器会从主存中一次性加载cache line大小的数据。 因此当执行arr[0][0] = 1时，缓存控制器发现arr[0][0]不在缓存中，也就是缓存不命中，需要去主存中进行读取并将数据加载到缓存中。假设此时cache line的大小为64字节，会从主存中读取arr[0][0]到arr[0][15]的数据到内存中。当执行访问arr[0][1]时，在缓存中能找到其位置，也就是缓存命中，此时的访问速度较快。 而对于代码2，当执行arr[0][0] = 1时，缓存控制器同样发现arr[0][0]不在缓存中，需要去主存中进行读取并将数据加载到缓存中，同样也会从主存中读取arr[0][0]到arr[0][15]的数据到内存中。当第二轮读取arr[1][0]时依然会发生缓存不命中，并且一直到arr[99][0]都是不命中。那么当访问arr[0][1]会是怎么情况呢？ 此时就需要考虑缓存的大小了。如果缓存大小大于数组arr大小，缓存此时相当于缓存了整个arr数组的内容。那么后续访问其他元素，确实是缓存命中。似乎和代码1分析结果差不多。但是如果缓存的大小很小，例如只有数组一半大小，那么缓存命中率就很明显会降低。同样的缓存大小，代码1依然会获得很高的缓存命中率。","link":"/2021/04/01/cache/"},{"title":"Transformer","text":"我已经无法辨别这个世界的真假了 &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; —— Noth1n9 最近经常看到Transformer在计算机视觉各个领域中屠榜，因此突发奇想对其进行‘深入’研究。 虽然对其早有所耳闻，但也只是局限于《Attention is all you need》、Key、Value、Query等，于是‘奋发图强’打开Google，开始博览群博客。 Transformer模型框架 首先，Transformer的整体模型框架如上所示。整体同样采用了Encoder-Decoder的结构，Encoder由堆叠多个相同的Encoder模块组成，Decoder也同样如此，并且在Transformer中的Attention模块都是在self-attention的基础上形成的。 Encoder模块 Encoder模块主要由两个子模块组成：Multi-Head Attention和Feed Forward，并且在每个模块之后都有一个残差连接和正则化层。 Self-Attention 作者在原论文中将该注意力方式称为Scaled Dot-Product Attention，计算方式如上图所示。 首先，需要将模型的输入x分别映射成Query、Key和Value矩阵，这里在实现上通过将输入和三个不同的权重矩阵相乘来得到对应的Q,K和V矩阵，接着便可通过如下公式来计算自注意力的值：$$\\operatorname{Attention}(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}\\right) V$$该计算方式和Non-local中用来获得全局注意力的方式类似，先利用Query和Key矩阵之间的相关关系来表示矩阵中每个位置与其他位置的相关关系，可通过Q、K矩阵相乘得到，再通过softmax函数进行归一化。接着 使用归一化后的权重矩阵和Value矩阵相乘便可得到该注意力层的输出。 由于当Embedding维度过大时，矩阵乘法得到结果容易出现在softmax函数的饱和区，梯度较小，因此会除以$\\sqrt{d_k}$进行缩放。 Multi-Head Attention 作者在单个Self-Attention模块的基础上进行了扩展，提出了Multi-Head Attention模块，从上图中可以看出是对多个Self-Attention模块进行并行的计算处理，从对其结果进行拼接后通过一个线性层得到最终的结果。 $$ \\begin{aligned} \\operatorname{MultiHead}(Q, K, V) &=\\text { Concat }\\left(\\text { head }_{1}, \\ldots, \\text { head }_{\\mathrm{h}}\\right) W^{O} \\\\ \\quad \\text { where head }_{\\mathrm{i}} &=\\text { Attention }\\left(Q W_{i}^{Q}, K W_{i}^{K}, V W_{i}^{V}\\right) \\end{aligned} $$ 同时为了避免计算成本过多的增加，会先减少每个Self-Attention模块的大小。假设h=8，那么每个Self-Attention模块的维度也除以8。 Feed Forward Network该模块较为简单，有两层全连接组成，并在中间包含一个Relu激活层。同时，在论文中设置输入输出的维度为512，中间层的维度为2048。 单个Encoder模块的组成大概就是上面所述，通过堆叠多个Encoder模块，便可得到Transformer的Encoder输出，再将此输入到后续的Decoder中。 Decoder模块 在Decoder模块中由三个子模块组成：Masked Multi-Head Attention、Multi-Head Attention和Feed Forward，在每个模块之后也都有一个残差连接和正则化层。 Masked Multi-Head AttentionMasked Multi-Head Attention模块顾名思义就是在Encoder中的Multi-Head Attention模块基础上添加了一个Mask层。 那么为什么在Decoder中需要对Attention层加入Mask呢？ 由于在序列问题中，对于输入我们能直接获得完整的输入，但是输出的具有顺序的，我们需要逐个token预测输出，因此在预测当前输出token时，是无法使用到未来的信息，只能使用到过去信息。在训练过程中，虽然我们可以手动处理来得到成对的输入输出，但这么做会导致每个训练数据的长度不同，而在深度学习中往往是对一个batch的数据进行训练，这是就可以添加一个Mask对未来信息进行遮挡，使得数据的长度能保持一致，拼接成一个batch的数据进行训练。在实现上通过一个如下所示的Mask矩阵，再将Attention输入矩阵根据Mask中对应为0的位置设为负无穷来达到遮挡未来信息的效果。 123456789101112tensor([[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]], dtype=torch.uint8) scores = scores.masked_fill(mask == 0, -1e9) Multi-Head AttentionDecoder模块中的Multi-Head Attention计算方式也和Encoder中的计算方式相同，但数据的来源有所不同。 在Encoder模块中所有输入都是由上一个模块的同一个输出得到的，而在Decoder中， 从模型结构图也可以看出输入中的Key和Value是来自Encoder模块的，这也是Encoder和Decoder连接的主要方式(不太确定是不是唯一方式，嘻嘻)。 Feed Forward Network这里Feed Forward Network的结构可以说是和Encoder一模一样滴。 在通过堆叠的多个Decoder模块之后，通过一个线性层映射到输出语料的维度，再通过softmax函数归一化获得下一个token的概率。这样就得到了整个Transformer模型单次预测的输出。 Positional Encoding整理完Encoder和Decoder模块之后，可以发现输入在经过Embedding之后还通过一个Positional Encoding模块。 由于Transformer中的Attention模块不同于RNN、LSTM具有循环结构，在Attention计算过程中是没有使用到每个token之间的相对或绝对位置关系。因此在Encoder和Decoder的embedding层之后加入Positional Encoding对位置信息进行编码。 在论文中，作者使用sin和cos函数来对位置信息进行编码：$$\\begin{aligned}P E_{(p o s, 2 i)} &amp;=\\sin \\left(p o s / 10000^{2 i / d_{\\text {model }}}\\right) \\\\P E_{(p o s, 2 i+1)} &amp;=\\cos \\left(p o s / 10000^{2 i / d_{\\text {model }}}\\right)\\end{aligned}$$其中$pos$表示该token在句子中的位置，而$i$表示维度。因此得到的位置编码结果和经过Embedding之后的向量具有相同的维度，可直接相加。之所以选择余弦函数是为了能获得相对位置关系，这里引用原论文中的话： We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset $k$, $PEpos+k$ can be represented as a linear function of $PEpos$. 模型的输入输出虽然整个模型的结构框架已经有了详细的了解，接下来分析模型的输入输出分别是什么。 该模型在论文中主要对应的是文本翻译任务，那么我们假设一对数据： 我爱你➡I love you， 从上图中可以看出在Encoder中有一个输入，也就是输入序列：我爱你，并将其通过Embedding映射成模型输入数据。 而在Decoder中，可以看到模型的输入不仅有来自Encoder的输出，还有一个$Output$，并同时会输出当前的一个概率。由于Decoder处理的是一个序列问题，需要逐个预测出输出。因此，在预测当前token的时候，能够将先前预测得到的token序列作为Decoder的输入来预测当前的token。 那么在预测第一个输出时Decoder的输入应该是啥呢？这时就是Shifted right起作用的时候了，这时会使用一个起始符号&lt;/s&gt;或其他自定义符号作为输入来得到第一个真实的输出token。因此我们可以将整个过程整理如下： Step 1 输入：我爱你 $\\rightarrow$ Encoder 输出：Encoder特征 Step 2 输入：Encoder特征 + 起始符号&lt;/s&gt; $\\rightarrow$ Decoder 输出：产生预测 ’I‘ Step 3 输入：Encoder特征 + 起始符号&lt;/s&gt; + ’I‘ $\\rightarrow$ Decoder 输出：产生预测 ’love‘ Step 4 输入：Encoder特征 + 起始符号&lt;/s&gt; + ’I‘ + ’love‘ $\\rightarrow$ Decoder 输出：产生预测 ’you‘ 这样就得到了完整的输出序列。同时，为了缓解中间预测错误导致后续预测偏离问题，可采用Beam Search策略。 总结敬候佳音 参考连接https://zhuanlan.zhihu.com/p/82312421 https://zhuanlan.zhihu.com/p/48508221 https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html https://www.zhihu.com/question/344516091 https://www.cnblogs.com/zingp/p/11696111.html https://zhuanlan.zhihu.com/p/166608727","link":"/2021/04/07/transformer/"},{"title":"Git工具","text":"&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; —— Noth1n9 版本控制器的起源diff与patchdiff：diff用来比较两个文件或者目录之间的差异 patch：pathc是diff的反向操作，可以通过diff所生成的diff.txt差异文件做到两个原文件的互相转换 Git工具Git基本配置配置个人身份12git config --global user.name &quot;noth1n9&quot;gei config --global user.email 932874795@qq.com 这个配置信息会在Git仓库中提交的修改信息中体现，但和Gti服务器认证使用的密码或者公钥密码无关。 文本编码配置1234567#中文编码支持git config --global gui.encoding utf-8git config --global i18n.commitencoding utf-8git config --global i18n.logoutputencoding utf-8#显示路径中的中文git config --global core.quotepath utf-8 Git基本命令Git工作区域 版本库（Repository）：在工作区有一个隐藏目录.git，这个文件夹就是Git的版本库，里面存放了Git用来管理改工程的所有版本数据，也可以叫本地仓库。 工作区（Working Directory）：日常工作的代码文件或者文档所在的文件夹。 暂存区（stage）：一般存放在工程根目录.git/index文件中，所以我们也可以把暂存区叫做索引。 文件状态 已提交（committed）：该文件已经被安全地保存在本地数据库中了 已修改（modified）：修改了摸个文件，但还没有提交保存 已暂存（staged）：把已修改的文件放在下次提交时要保存的清单中 Git常用命令工程准备新建工程——git init *** 工程克隆——git clone 查看工作区查看工作区的修改内容——git diff 查看工作区文件状态——git status 文件修改后提交推送新增/删除/移动文件到暂存区——git add/ git rm/ git mv 提交更改的文件——git commit 推送远端仓库——git push 查看日志查看当前分支上的提交日志——git log 分支管理列出本地分支——git branch 新建分支——git branch /git checkout -b 删除分支——git branch -d 切换分支——git checkout 更新分支——git pull 合并分支——git merge 撤销操作强制回退到历史结点——git reset 回退本地所有修改而未提交的——git checkout 分支合并合并目标股分支内容到当前分支——git merge/ git rebase","link":"/2021/05/17/git/"}],"tags":[],"categories":[{"name":"操作系统","slug":"操作系统","link":"/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"name":"深度学习","slug":"深度学习","link":"/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"自然语言处理","slug":"深度学习/自然语言处理","link":"/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"},{"name":"语言工具","slug":"语言工具","link":"/categories/%E8%AF%AD%E8%A8%80%E5%B7%A5%E5%85%B7/"}]}